{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009203,
     "end_time": "2023-09-01T16:50:47.024469",
     "exception": false,
     "start_time": "2023-09-01T16:50:47.015266",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Attention!!!\n",
    "\n",
    "This is a very simple but bad quality notebook. \n",
    " - I do not use any sort of ranking loss, which would be better.\n",
    " - My strategy instead is to min-max scale the times and apply L1-loss\n",
    " - My model is also not optimized. It is a relatively simple GNN that embeds the graph and only processes 1 datapoint at a time and is only trained on 1 epoch.\n",
    " - The public score would be much better if you paired this submission with a trained model for layout. Since this only contributes to half of the score.\n",
    " - Have fun playing around with it!\n",
    " \n",
    " \n",
    " # CHANGES\n",
    " - V5 - normalized train and infer targets, use MSE loss, changed evaluation metric to perform top5 mean instead of top5 max for robustness, 5-fold CV\n",
    " - V6 - use SAGEConv instead of GCN, add dropout layer, increase number of paramters, changed evaluation metric to perform top50 mean, 10->20 epochs.\n",
    " - V13 - fixed problem where model weights weren't being reset leading to heavy overfitting...oops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-output": true,
    "papermill": {
     "duration": 264.245878,
     "end_time": "2023-09-01T16:55:11.276628",
     "exception": false,
     "start_time": "2023-09-01T16:50:47.03075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: torch-geometric in /root/miniconda3/lib/python3.8/site-packages (2.3.1)\n",
      "Requirement already satisfied: torch-scatter in /root/miniconda3/lib/python3.8/site-packages (2.1.1)\n",
      "Requirement already satisfied: scikit-learn in /root/miniconda3/lib/python3.8/site-packages (from torch-geometric) (1.3.1)\n",
      "Requirement already satisfied: scipy in /root/miniconda3/lib/python3.8/site-packages (from torch-geometric) (1.10.1)\n",
      "Requirement already satisfied: numpy in /root/miniconda3/lib/python3.8/site-packages (from torch-geometric) (1.24.2)\n",
      "Requirement already satisfied: pyparsing in /root/miniconda3/lib/python3.8/site-packages (from torch-geometric) (3.0.9)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /root/miniconda3/lib/python3.8/site-packages (from torch-geometric) (5.9.4)\n",
      "Requirement already satisfied: tqdm in /root/miniconda3/lib/python3.8/site-packages (from torch-geometric) (4.61.2)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.8/site-packages (from torch-geometric) (3.1.2)\n",
      "Requirement already satisfied: requests in /root/miniconda3/lib/python3.8/site-packages (from torch-geometric) (2.28.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/lib/python3.8/site-packages (from jinja2->torch-geometric) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.8/site-packages (from requests->torch-geometric) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.8/site-packages (from requests->torch-geometric) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /root/miniconda3/lib/python3.8/site-packages (from requests->torch-geometric) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.8/site-packages (from requests->torch-geometric) (2.10)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /root/miniconda3/lib/python3.8/site-packages (from scikit-learn->torch-geometric) (3.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /root/miniconda3/lib/python3.8/site-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-geometric torch-scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 4.819384,
     "end_time": "2023-09-01T16:55:16.104784",
     "exception": false,
     "start_time": "2023-09-01T16:55:11.2854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm \n",
    "\n",
    "import sklearn,sklearn.model_selection\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch_geometric.nn import GCNConv,SAGEConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 0.020227,
     "end_time": "2023-09-01T16:55:16.152594",
     "exception": false,
     "start_time": "2023-09-01T16:55:16.132367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_df(directory):\n",
    "    splits = [\"train\", \"valid\", \"test\"]\n",
    "    dfs = dict()\n",
    "    \n",
    "    for split in splits:\n",
    "        path = os.path.join(directory, split)\n",
    "        files = os.listdir(path)\n",
    "        list_df = []\n",
    "        \n",
    "        for file in files:\n",
    "            d = dict(np.load(os.path.join(path,file)))\n",
    "            d['file'] = file\n",
    "            list_df.append(d)\n",
    "        dfs[split] = pd.DataFrame.from_dict(list_df)\n",
    "    return dfs\n",
    "tile_xla = load_df(\"/root/autodl-tmp/npz_all/npz/tile/xla/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008874,
     "end_time": "2023-09-01T16:56:21.968592",
     "exception": false,
     "start_time": "2023-09-01T16:56:21.959718",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 0.020329,
     "end_time": "2023-09-01T16:56:21.997734",
     "exception": false,
     "start_time": "2023-09-01T16:56:21.977405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TileDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        config_feat = torch.tensor(row['config_feat'].astype(np.float32))\n",
    "        node_feat = torch.tensor(row['node_feat'].astype(np.float32))\n",
    "        node_opcode = torch.tensor(row['node_opcode'].astype(np.int64))\n",
    "        edge_index = torch.tensor(np.swapaxes(row['edge_index'],0,1).astype(np.int64))\n",
    "        target = (row['config_runtime']/(row['config_runtime_normalizers']+1e-5)).astype(np.float32) #/row['config_runtime_normalizers']\n",
    "        # minmax scale the target, we only care about order\n",
    "        target = (target-np.mean(target))/(np.std(target)+1e-5)\n",
    "\n",
    "#         target = (target-np.mean(target))/(np.std(target))\n",
    "        target = torch.tensor(target)\n",
    "        return config_feat,node_feat,node_opcode,edge_index,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedModel(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, graph_in, graph_out, hidden_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        op_embedding_dim = 4\n",
    "        self.embedding = torch.nn.Embedding(120, op_embedding_dim)\n",
    "        \n",
    "        self.linear = nn.Linear(op_embedding_dim + 140, graph_in)\n",
    "        \n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.norms = torch.nn.ModuleList()\n",
    "        \n",
    "        in_channels = graph_in\n",
    "        for out_channels in hidden_channels:\n",
    "            self.convs.append(SAGEConv(in_channels, out_channels))\n",
    "            self.norms.append(nn.LayerNorm(out_channels))\n",
    "            in_channels = out_channels\n",
    "            \n",
    "        self.convs.append(SAGEConv(in_channels, graph_out))\n",
    "        \n",
    "        self.dense = torch.nn.Sequential(\n",
    "            nn.Linear(graph_out * 2 + 24, hidden_dim),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_cfg, x_feat, x_op, edge_index):\n",
    "        x = torch.cat([x_feat, self.embedding(x_op)], dim=1)\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        for conv, norm in zip(self.convs[:-1], self.norms):\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "            x = norm(x)\n",
    "        \n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        \n",
    "        x_mean = x.mean(0)\n",
    "        x_max = x.max(0).values\n",
    "        \n",
    "        x = torch.cat([x_cfg, x_max.repeat((len(x_cfg), 1)), x_mean.repeat((len(x_cfg), 1))], dim=1)\n",
    "        x = torch.flatten(self.dense(x))\n",
    "        x = (x - torch.mean(x)) / (torch.std(x) + 1e-5)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008439,
     "end_time": "2023-09-01T16:56:22.088164",
     "exception": false,
     "start_time": "2023-09-01T16:56:22.079725",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train One Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "papermill": {
     "duration": 0.021726,
     "end_time": "2023-09-01T16:56:22.11899",
     "exception": false,
     "start_time": "2023-09-01T16:56:22.097264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.concat((tile_xla[\"train\"],tile_xla[\"valid\"]),axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_tile_mean(predictions, df):\n",
    "    score = 0\n",
    "    for i in range(len(df)):\n",
    "        predbest = np.mean(df.iloc[i]['config_runtime'][predictions[i]])\n",
    "        best = np.mean(np.sort(df.iloc[i]['config_runtime'])[:50])\n",
    "        score += 2-predbest/best\n",
    "    score /= len(df)\n",
    "    return score\n",
    "\n",
    "def score_tile_max(predictions, df):\n",
    "    score = 0\n",
    "    for i in range(len(df)):\n",
    "        predbest = np.min(df.iloc[i]['config_runtime'][predictions[i][:5]])\n",
    "        best = np.min(df.iloc[i]['config_runtime'])\n",
    "        score += 2 - predbest/best\n",
    "    score /= len(df)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping on fold 0 at epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping on fold 1 at epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping on fold 2 at epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running loss: 0.38, current loss: 0.27:  96%|█████████▋| 4929/5108 [01:01<00:02, 76.07it/s] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping on fold 3 at epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running loss: 0.31, current loss: 1.18:  74%|███████▎  | 3757/5108 [00:46<00:18, 72.32it/s] "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sklearn.model_selection\n",
    "\n",
    "# Assuming you've defined SimpleModel and TileDataset classes above\n",
    "# Also assuming you've defined CosineLRScheduler\n",
    "\n",
    "# Data loading and preprocessing\n",
    "df = pd.concat((tile_xla[\"train\"], tile_xla[\"valid\"]), axis=0).reset_index(drop=True)\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "kfold = sklearn.model_selection.KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "score_means = []\n",
    "score_maxs = []\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5\n",
    "counter = 0\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(kfold.split(df)):\n",
    "    model = EnhancedModel(hidden_channels=[32, 48, 64, 84], graph_in=64, graph_out=64, hidden_dim=128, dropout=0.2).to(device)\n",
    "    train_dataset = TileDataset(df.iloc[tr_idx])\n",
    "    val_dataset = TileDataset(df.iloc[va_idx])\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    steps = len(train_dataset) * 20\n",
    "    warmup_steps = int(steps * 0.2)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    scheduler = CosineLRScheduler(optimizer, t_initial=steps, warmup_t=warmup_steps, warmup_lr_init=1e-6, lr_min=2e-8)\n",
    "    \n",
    "    best_score = 0\n",
    "    best_score_max = 0\n",
    "    \n",
    "    for epoch in range(50):  # Set max epochs to 50\n",
    "        # Training\n",
    "        model.train()\n",
    "        pbar = tqdm(range(len(train_dataset)), leave=False)\n",
    "        loss_sum = 0\n",
    "        n = 0\n",
    "        for i in pbar:\n",
    "            cfg_ft, nd_ft, nd_op, ind, target = train_dataset[i]\n",
    "            cfg_ft, nd_ft, nd_op, ind, target = cfg_ft.to(device), nd_ft.to(device), nd_op.to(device), ind.to(device), target.to(device)\n",
    "            \n",
    "            out = model(cfg_ft, nd_ft, nd_op, ind)\n",
    "            loss = criterion(out, target)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1e-2)\n",
    "            scheduler.step(i + len(train_dataset) * epoch)\n",
    "            optimizer.step()\n",
    "            loss_sum += loss.item()\n",
    "            n += 1\n",
    "            pbar.set_description(f'running loss: {(loss_sum / n):.2f}, current loss: {(loss.item()):.2f}')\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        tile_xla_predictions = []\n",
    "        pbar = tqdm(range(len(val_dataset)), leave=False)\n",
    "        for i in pbar:\n",
    "            cfg_ft, nd_ft, nd_op, ind, target = val_dataset[i]\n",
    "            cfg_ft, nd_ft, nd_op, ind, target = cfg_ft.to(device), nd_ft.to(device), nd_op.to(device), ind.to(device), target.to(device)\n",
    "            \n",
    "            out = model(cfg_ft, nd_ft, nd_op, ind)\n",
    "            tile_xla_predictions.append(np.argsort(out.detach().cpu().numpy())[:50])\n",
    "        \n",
    "        score_mean = score_tile_mean(tile_xla_predictions, val_dataset.df)\n",
    "        score_max = score_tile_max(tile_xla_predictions, val_dataset.df)\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if score_mean > best_score:\n",
    "            best_score = score_mean\n",
    "            best_score_max = score_max\n",
    "            torch.save(model.state_dict(), f'best_model_{fold}.pth')\n",
    "            counter = 0  # Reset counter\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping on fold {fold} at epoch {epoch}\")\n",
    "                break  # Stop if counter reaches patience\n",
    "    \n",
    "    score_means.append(best_score)\n",
    "    score_maxs.append(best_score_max)\n",
    "\n",
    "# Final score\n",
    "print(f'comp_score = {np.mean(score_maxs)}, mean_score = {np.mean(score_means)},')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 9138.052878,
     "end_time": "2023-09-01T19:28:40.180685",
     "exception": false,
     "start_time": "2023-09-01T16:56:22.127807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# kfold = sklearn.model_selection.KFold(n_splits=5,shuffle=True,random_state=0)\n",
    "# score_means = []\n",
    "# score_maxs = []\n",
    "# for fold,(tr_idx,va_idx) in enumerate(kfold.split(df)):\n",
    "#     model = EnhancedModel(hidden_channels = [32,48,64,84],graph_in = 64,graph_out = 64,hidden_dim=128,dropout = 0.2).to(device)\n",
    "#     train_dataset = TileDataset(df.iloc[tr_idx])\n",
    "#     val_dataset = TileDataset(df.iloc[va_idx])\n",
    "#     criterion = torch.nn.MSELoss()\n",
    "#     steps = len(train_dataset)*20\n",
    "#     warmup_steps = int(steps*0.2)\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=1e-4,weight_decay = 1e-4)\n",
    "#     scheduler = CosineLRScheduler(optimizer,t_initial= steps,warmup_t=warmup_steps, warmup_lr_init=1e-6,lr_min=2e-8,)\n",
    "    \n",
    "#     def score_tile_mean(predictions, df):\n",
    "#         score = 0\n",
    "#         for i in range(len(df)):\n",
    "#             predbest = np.mean(df.iloc[i]['config_runtime'][predictions[i]])\n",
    "#             best = np.mean(np.sort(df.iloc[i]['config_runtime'])[:50])\n",
    "#             score += 2-predbest/best\n",
    "#         score /= len(df)\n",
    "#         return score\n",
    "#     def score_tile_max(predictions, df):\n",
    "#         score = 0\n",
    "#         for i in range(len(df)):\n",
    "#             predbest = np.min(df.iloc[i]['config_runtime'][predictions[i][:5]])\n",
    "#             best = np.min(df.iloc[i]['config_runtime'])\n",
    "#     #         print(best,predbest)\n",
    "#             score += 2 - predbest/best\n",
    "#         score /= len(df)\n",
    "#         return score\n",
    "\n",
    "#     best_score = 0\n",
    "#     best_score_max = 0\n",
    "#     for epoch in range(10):\n",
    "#         model.train()\n",
    "#         pbar = tqdm(range(len(train_dataset)),leave=False)\n",
    "#         loss_sum = 0\n",
    "#         n = 0\n",
    "#         for i in pbar:\n",
    "#             cfg_ft,nd_ft,nd_op,ind,target = train_dataset[i]\n",
    "#             cfg_ft,nd_ft,nd_op,ind,target = cfg_ft.to(device),nd_ft.to(device),nd_op.to(device),ind.to(device),target.to(device)\n",
    "\n",
    "#             out = model(cfg_ft,nd_ft,nd_op,ind)\n",
    "#             loss = criterion(out, target)\n",
    "#             loss.backward()\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), 1e-2)\n",
    "#             scheduler.step(i+len(train_dataset)*epoch)\n",
    "#             optimizer.step()\n",
    "#             loss_sum+=loss.item()\n",
    "#             n+=1\n",
    "#             pbar.set_description(f'running loss: {(loss_sum/n):.2f},current loss: {(loss.item()):.2f}')\n",
    "#         pbar.close()\n",
    "#         model.eval()\n",
    "\n",
    "#         tile_xla_predictions = []\n",
    "#         pbar = tqdm(range(len(val_dataset)),leave=False)\n",
    "#         for i in pbar:\n",
    "#             cfg_ft,nd_ft,nd_op,ind,target = val_dataset[i]\n",
    "#             cfg_ft,nd_ft,nd_op,ind,target = cfg_ft.to(device),nd_ft.to(device),nd_op.to(device),ind.to(device),target.to(device)\n",
    "\n",
    "#             out = model(cfg_ft,nd_ft,nd_op,ind)\n",
    "#             # tile_xla_predictions.append(np.argsort(out.detach().numpy())[:50])\n",
    "#             tile_xla_predictions.append(np.argsort(out.detach().cpu().numpy())[:50])\n",
    "#         pbar.close()\n",
    "#         score_mean = score_tile_mean(tile_xla_predictions, val_dataset.df)\n",
    "#         score_max = score_tile_max(tile_xla_predictions, val_dataset.df)\n",
    "#         print(f'fold {fold} epoch {epoch}, comp_score = {score_max:.3f}, mean_score = {score_mean:.3f},')\n",
    "#         if score_mean>best_score:\n",
    "#             best_score = score_mean\n",
    "#             best_score_max = score_max\n",
    "#             torch.save(model.state_dict(), f'best_model_{fold}.pth')\n",
    "#     score_means.append(best_score)\n",
    "#     score_maxs.append(best_score_max)\n",
    "# print(f'comp_score = {np.mean(score_maxs)}, mean_score = {np.mean(score_means)},')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 20.903162,
     "end_time": "2023-09-01T19:29:21.465429",
     "exception": false,
     "start_time": "2023-09-01T19:29:00.562267",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluate on Validation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 20.540632,
     "end_time": "2023-09-01T19:30:02.340577",
     "exception": false,
     "start_time": "2023-09-01T19:29:41.799945",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**0.31 is not bad considering that this model only trained on 1 epoch and is not on a ranking loss!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 20.552961,
     "end_time": "2023-09-01T19:30:43.664106",
     "exception": false,
     "start_time": "2023-09-01T19:30:23.111145",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predict and Submit (only tile:xla predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 42.864288,
     "end_time": "2023-09-01T19:31:46.765649",
     "exception": false,
     "start_time": "2023-09-01T19:31:03.901361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = TileDataset(tile_xla[\"test\"])\n",
    "tile_xla_predictions = [[] for i in range(len(dataset))]\n",
    "for fold in range(5):\n",
    "    model.load_state_dict(torch.load(f'/root/autodl-tmp/best_model_{fold}.pth'))\n",
    "    model.eval()\n",
    "    pbar = tqdm(range(len(dataset)))\n",
    "    for i in pbar:\n",
    "        cfg_ft,nd_ft,nd_op,ind,target = dataset[i]\n",
    "        cfg_ft,nd_ft,nd_op,ind,target = cfg_ft.to(device),nd_ft.to(device),nd_op.to(device),ind.to(device),target.to(device)\n",
    "\n",
    "        out = model(cfg_ft,nd_ft,nd_op,ind)\n",
    "        tile_xla_predictions[i].append(out.detach().cpu().numpy())\n",
    "tile_xla_predictions = [np.argsort(np.mean(pred,axis=0))[:5] for pred in tile_xla_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 20.880172,
     "end_time": "2023-09-01T19:32:28.307392",
     "exception": false,
     "start_time": "2023-09-01T19:32:07.42722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('/root/autodl-tmp/sample_submission.csv')\n",
    "for i,filename in enumerate(tile_xla[\"test\"]['file'].values):\n",
    "    id = 'tile:xla:' +filename[:-4]\n",
    "    sub.loc[sub.ID == id,'TopConfigs'] = ';'.join(tile_xla_predictions[i].astype(str))\n",
    "sub.to_csv('submission.csv',index=False)\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9735.485807,
   "end_time": "2023-09-01T19:32:52.176348",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-09-01T16:50:36.690541",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
